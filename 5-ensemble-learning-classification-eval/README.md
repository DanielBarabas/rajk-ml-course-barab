# Feature engineering techniques for categorical data

check <a href=https://github.com/morkapronczay/rajk-ml-course/blob/main/6-ensemble-learning/create-toy-dataset.ipynb>notebook</a> with the example on the generated toy dataset.

# Condorcet Jury Theorem


another aspect of ensemble learning, check <a href=https://github.com/morkapronczay/rajk-ml-course/blob/main/6-ensemble-learning/Condorcet-Jury-Theorem.ipynb>notebook</a>.

# ensemble-blogpost
Here I reuse some previous materials I created to talk about ensemble learning foundations (I) and practices (II).

Part 1: <a href="https://medium.com/starschema-blog/combine-your-machine-learning-models-for-better-out-of-sample-accuracy-14f0f60ffe13">Combine your machine learning models for better out-of-sample accuracy</a>

This part explores bootstrapping, as a theoretical foundation of ensemble learning.

The first part is coded out in <a href=https://github.com/morkapronczay/ensemble-blogpost/blob/master/bootstrap.ipynb>bootstrapping.ipynb</a>

Part 2: <a href="https://medium.com/starschema-blog/digging-deeper-into-ensemble-learning-7d218be8cf00">Digging deeper into ensemble learning</a>

This part outlines the concepts of bagging and boosting, and how they help you get better out-of-sample fit for your Machine Learning model.

The second part is coded out in <a href=https://github.com/morkapronczay/ensemble-blogpost/blob/master/bagging-boosting.ipynb>bagging-boosting.ipynb</a>

You can check out the links in the blogposts for further reading!

# the Lapker case

we talked about how different modelling approaches can be combined (cross-section, time-series, recommender system) in the spirit of ensemble learning.
